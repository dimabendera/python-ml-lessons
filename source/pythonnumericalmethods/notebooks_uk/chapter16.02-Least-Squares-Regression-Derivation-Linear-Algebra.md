<h1>Виведення регресії методом найменших квадратів (Лінійна алгебра)<a href="#least-squares-regression-derivation-linear-algebra" title="Постійне посилання на цей заголовок"></a></h1>
<p>По-перше, ми перераховуємо оцінку даних у кожній точці даних <span>\(x_i\)</span></p>

\[\begin{eqnarray*}
&amp;&amp;\hat{y}(x_1) = {\alpha}_1 f_1(x_1) + {\alpha}_2 f_2(x_1) + \cdots + {\alpha}_n f_n(x_1), \\
&amp;&amp;\hat{y}(x_2) = {\alpha}_1 f_1(x_2) + {\alpha}_2 f_2(x_2) + \cdots + {\alpha}_n f_n(x_2), \\
&amp;&amp;\qquad\qquad \qquad \qquad \quad \cdots\\
&amp;&amp;\hat{y}(x_m) = {\alpha}_1 f_1(x_m) + {\alpha}_2 f_2(x_m) + \cdots + {\alpha}_n f_n(x_m).\end{eqnarray*}\]
<p>Нехай <span>\(X\in {\Bbb R}^n\)</span> — вектор-стовпець, такий що <span>\(i\)</span>-й елемент <span>\(X\)</span> містить значення <span>\(i\)</span>-ї точки даних <span>\(x\)</span>, <span>\(x_i\)</span>, <span>\(\hat{Y}\)</span> — вектор-стовпець з елементами <span>\(\hat{Y}_i = \hat{y}(x_i)\)</span>, <span>\({\beta}\)</span> — вектор-стовпець, такий що <span>\({\beta}_i = {\alpha}_i\)</span>, <span>\(F_i(x)\)</span> — функція, яка повертає вектор-стовпець <span>\(f_i(x)\)</span>, обчислений для кожного елемента <span>\(x\)</span>, а <span>\(A\)</span> — матриця розміром <span>\(m \times n\)</span>, така що <span>\(i\)</span>-й стовпець <span>\(A\)</span> — це <span>\(F_i(x)\)</span>. За такого позначення попередня система рівнянь стає <span>\(\hat{Y} = A{\beta}\)</span>.</p>
<p>Тепер, якщо <span>\(Y\)</span> — вектор-стовпець, такий що <span>\(Y_i = y_i\)</span>, то загальна сума квадратів похибок задається як <span>\(E = \|{\hat{Y} - Y}\|_{2}^2\)</span>. Ви можете перевірити це, підставивши визначення <span>\(L_2\)</span>-норми. Оскільки ми хочемо зробити <span>\(E\)</span> якомога меншим, а норми є мірою відстані, цей попередній вираз еквівалентний твердженню, що ми хочемо, щоб <span>\(\hat{Y}\)</span> і <span>\(Y\)</span> були "якомога ближчими". Зауважте, що загалом <span>\(Y\)</span> не буде в області значень <span>\(A\)</span>, і тому <span>\(E > 0\)</span>.</p>
<p>Розглянемо наступне спрощене зображення області значень <span>\(A\)</span>; дивіться наступний рисунок. Зауважте, що це <span>\(\it не\)</span> графік точок даних <span>\((x_i, y_i)\)</span>.</p>

<p>З спостереження, вектор в області значень <span>\(A, \hat{Y}\)</span>, який є найближчим до <span>\(Y\)</span>, є тим, що може бути перпендикулярним до <span>\(Y\)</span>. Отже, ми хочемо вектор <span>\(Y - \hat{Y}\)</span>, який є перпендикулярним до вектора <span>\(\hat{Y}\)</span>.</p>
<p>Нагадаємо з лінійної алгебри, що два вектори є перпендикулярними, або ортогональними, якщо їх скалярний добуток дорівнює 0. Зауважуючи, що скалярний добуток між двома векторами, <span>\(v\)</span> і <span>\(w\)</span>, можна записати як <span>\({\text{dot}}(v,w) = v^T w\)</span>, ми можемо стверджувати, що <span>\(\hat{Y}\)</span> і <span>\(Y - \hat{Y}\)</span> є перпендикулярними, якщо <span>\({\text{dot}}(\hat{Y}, Y - \hat{Y}) = 0\)</span>; отже, <span>\(\hat{Y}^T (Y - \hat{Y}) = 0\)</span>, що еквівалентно <span>\((A{\beta})^T(Y - A{\beta}) = 0\)</span>.</p>
<p>Зауважуючи, що для двох матриць <span>\(A\)</span> і <span>\(B, (AB)^T = B^T A^T\)</span> і використовуючи дистрибутивні властивості множення векторів, це еквівалентно <span>\({\beta}^T A^T Y - {\beta}^T A^T A {\beta} = {\beta}^T(A^T Y - A^T A {\beta}) = 0\)</span>. Розв'язок, <span>\({\beta} = \textbf{0}\)</span>, є тривіальним, тому ми використовуємо <span>\(A^T Y - A^T A {\beta} = 0\)</span> для знаходження більш цікавого розв'язку. Розв'язання цього рівняння для <span>\({\beta}\)</span> дає <span>\(\textbf{формулу регресії методом найменших квадратів}\)</span>:</p>

\[
{\beta} = (A^T A)^{-1} A^T Y
\]
<p>Зауважте, що <span>\((A^T A)^{-1}A^T\)</span> називається <strong>псевдооберненою матрицею</strong> для <span>\(A\)</span> і існує, коли <span>\(m > n\)</span> і <span>\(A\)</span> має лінійно незалежні стовпці. Доведення оборотності <span>\((A^T A)\)</span> виходить за рамки нашого курсу, але вона завжди оборотна, за винятком деяких патологічних випадків.</p>
